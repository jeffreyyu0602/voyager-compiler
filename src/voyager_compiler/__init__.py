import operator

from google.protobuf import text_format

import torch.nn as nn
import torch.nn.functional as F
from torch._subclasses.fake_tensor import FakeTensorMode
from torch.utils._pytree import tree_flatten

from .codegen import *
from .decomposed import *
from .fake_quantize import *
from .fp8 import *
from .histogram import *
from .llm_utils import *
from .normal_float import *
from .posit import *
from .pt2e_utils import *
from .qconfig import *
from .quantize import *
from .quantize_pt2e import *
from .quantizer import *
from .training_args import *
from .utils import *


__all__ = [
    "FusedAmaxObsFakeQuantize",
    "QConfig",
    "QuantizationSpec",
    "TorchExportableModuleWithStaticCache",
    "add_qspec_args",
    "convert_and_export_with_split_cache",
    "convert",
    "deduplicate_nodes",
    "derive_bias_qparams_fn",
    "dispatch_model",
    "dtype_byte_size",
    "export_model",
    "fetch_attr",
    "generate",
    "get_aten_graph_module",
    "get_device_map",
    "get_node_name_to_scope",
    "get_qconfig",
    "get_default_quantizer",
    "insert_align_device_nodes",
    "plot_histogram",
    "plot_layer_range",
    "prepare",
    "prepare_pt2e",
    "print_node_scope_tabular",
    "propagate_config",
    "propagate_shape",
    "quantize",
    "quantize_to_fp8_e4m3",
    "quantize_to_fp8_e5m2",
    "quantize_to_nf",
    "quantize_to_posit",
    "replace_softmax",
    "sink_obs_or_fq",
    "swap_llama_attention",
    "with_execution_context",
]

class qscheme: ...

# Defined in voyager_compiler/quantizer.h
per_tensor_symmetric: qscheme = QScheme.PER_TENSOR_SYMMETRIC
per_channel_symmetric: qscheme = QScheme.PER_CHANNEL_SYMMETRIC
microscaling: qscheme = QScheme.MICROSCALING
group_wise_affine: qscheme = QScheme.GROUP_WISE_AFFINE


class OpMatcher:
    targets: Tuple[Callable, ...]
    predicate: Optional[Callable[[Node], bool]] = None

    def __init__(self, *ops, predicate=None):
        self.predicate = predicate

        # Resolve symbolic ops
        targets = []
        for op in ops:
            targets.extend(OPERATOR_MAPPINGS.get(op, [op]))

        # Freeze resolved targets
        self.targets = tuple(targets)

    def matches(self, node: Node) -> bool:
        if (source_fn_st := node.meta.get("source_fn_stack", None)) is not None:
            source_fn = source_fn_st[-1]
            if source_fn[1] not in self.targets:
                return False

        return self.predicate(node) if self.predicate else True


quantized_ops = torch.ops.quantized_ops
OPERATOR_MAPPINGS = {
    # Convolution and Linear
    "conv": [nn.Conv2d, F.conv2d],
    "gemm": [nn.Linear, F.linear, torch.matmul, operator.matmul],
    # Pooling
    "maxpool2d": [nn.MaxPool2d, F.max_pool2d],
    "avgpool2d": [nn.AdaptiveAvgPool2d, F.adaptive_avg_pool2d],
    # Arithmetic
    "add": ["add", "add_", operator.add, torch.add, operator.iadd],
    "sub": ["sub", "sub_", operator.sub, torch.sub, operator.isub],
    "mul": ["mul", "mul_", operator.mul, torch.mul, operator.imul],
    "div": ["div", "div_", operator.truediv, torch.div, operator.itruediv],
    # Non-Linear Activations
    "hardtanh" : [nn.ReLU6, F.relu6],
    "relu": [nn.ReLU, F.relu, F.relu_],
    "gelu": [nn.GELU, F.gelu],
    "sigmoid": [nn.Sigmoid, F.sigmoid],
    "silu": [nn.SiLU, F.silu],
    "tanh": [nn.Tanh, F.tanh],
    "softmax": ["softmax", nn.Softmax, F.softmax],
    # Normalization
    "layer_norm": ["layer_norm", nn.LayerNorm, F.layer_norm],
    # Quantization
    "quantize": [
        quantized_ops.quantize.default,
        quantized_ops.quantize_mx.default,
        quantized_ops.quantize_mx_outlier.default
    ],
    "dequantize": [quantized_ops.dequantize.default],
}


def fuse(
    model,
    fusion_pattern,
    example_args,
    example_kwargs=None,
    fuse_reshape=True,
    fake_mode=None
):
    if example_kwargs is None:
        example_kwargs = {}

    flatten_args, spec = tree_flatten((example_args, example_kwargs))
    ShapeProp(model, mode=fake_mode).propagate(*flatten_args)

    fuse_operator(model, fusion_pattern, fuse_reshape)
    return model


def transform(
    model: torch.fx.GraphModule,
    example_args,
    example_kwargs=None,
    patterns=None,
    unroll_dims=None,
    transform_layout=False,
    transpose_fc=False,
    cache_size=None,
    num_banks=None,
    fuse_operator=True,
    fuse_reshape=True,
    split_spmm=False,
    use_fake_mode=True
):
    if example_kwargs is None:
        example_kwargs = {}

    fake_mode = FakeTensorMode(allow_non_fake_inputs=True) if use_fake_mode else None

    flatten_args, spec = tree_flatten((example_args, example_kwargs))
    ShapeProp(model, mode=fake_mode).propagate(*flatten_args)

    # -------------------------------------------------------------------------
    # 1. Lowering & Decomposition
    # -------------------------------------------------------------------------
    # Break down complex operators (like MultiHeadAttention) into simpler
    # primitives and handle memory copy/concat operations.

    # split_multi_head_attention(model)

    # TODO Disabled for large models. This will be removed in the future once
    # we can handle stack/cat using DMA properly.
    if len(model.graph.nodes) < 10000:
        convert_expand_to_memory_copy(model)
        convert_cat_and_stack_as_stack_on_dim0(model)
        convert_cat_with_mismatched_shapes_to_stack(model)

    fuse_quantize_dequantize_with_previous_op(model)

    # -------------------------------------------------------------------------
    # 2. Hardware Alignment (Padding)
    # -------------------------------------------------------------------------
    # Pad dimensions to align with hardware unrolling constraints (SIMD, systolic
    # array dimensions, etc.) to ensure efficient execution.
    if unroll_dims is not None:
        pad_matrix_op_dimensions(model, *unroll_dims)
        pad_vector_op_dimensions(model, unroll_dims[1])

    # -------------------------------------------------------------------------
    # 3. Matrix Operation Tiling
    # -------------------------------------------------------------------------
    # Apply L2 tiling logic specifically for matrix operations (GEMM/Conv) to
    # optimize for the specific cache size and memory bank configuration.
    if cache_size is not None:
        run_matrix_op_l2_tiling(model, unroll_dims, cache_size, num_banks)

    # -------------------------------------------------------------------------
    # 4. Data Layout Transformation
    # -------------------------------------------------------------------------
    # Transform GEMM and convolution inputs/weights into layouts friendly
    # for systolic-array based hardware (e.g., transposing weights).

    if transform_layout:
        transpose_conv2d_inputs_and_weights(model)

    transpose_linear_weights(model, transform_layout, transpose_fc)

    ShapeProp(model, mode=fake_mode).propagate(*flatten_args)

    # Remove redundant reshapes that have no effect on tensor semantics
    eliminate_reshape_with_no_effect(model)

    # -------------------------------------------------------------------------
    # 5. Vector Operation Tiling
    # -------------------------------------------------------------------------
    # TODO: Used for unit test. Will be removed in the future
    from .codegen.passes.lowering import split_dense_spmm_node

    if split_spmm:
        split_dense_spmm_node(model)

    # Apply L2 tiling logic for vector-based operations.
    if cache_size is not None:
        run_vector_op_l2_tiling(model, unroll_dims[1], cache_size, num_banks)

    # -------------------------------------------------------------------------
    # 6. Operator Fusion
    # -------------------------------------------------------------------------
    # Perform final operator lowering and fuse sequences of operations (e.g.,
    # Conv+ReLU) into single kernels to reduce memory access overhead.

    if fuse_operator:
        fuse(
            model,
            patterns,
            flatten_args,
            fuse_reshape=fuse_reshape,
            fake_mode=fake_mode,
        )

    rename_nodes_with_param_names(model)
    deduplicate_nodes(model)

    return model


def compile(
    model: torch.fx.GraphModule,
    example_args,
    example_kwargs=None,
    total_memory=None,
    cache_size=None,
    num_banks=None,
    bank_width=None,
    unroll_dims=None,
    output_dir=None,
    output_file="compute_graph",
    dump_tensors=True,
    dump_snapshot=False,
):
    os.makedirs(output_dir, exist_ok=True)

    flatten_args, spec = tree_flatten((example_args, example_kwargs))
    ShapeProp(model).propagate(*flatten_args)

    allocator = MemoryAllocator(total_memory, bank_width=bank_width)
    run_memory_mapping(
        model, allocator, cache_size, num_banks, bank_width, unroll_dims
    )

    # Experimental feature
    from voyager_compiler.codegen.lowering.ir import Module
    from voyager_compiler.codegen.lowering.codegen import generate_proto

    top_module = Module.convert(model, name="m")
    print(top_module.format())

    params = generate_proto(top_module, model, flatten_args)

    with open(os.path.join(output_dir, 'module.txt'), "w") as f:
        f.write(text_format.MessageToString(params))

    if dump_snapshot:
        allocator.dump_snapshots(os.path.join(output_dir, "memory.png"))

    path = os.path.join(output_dir, "tensor_files")
    params = gen_code(model, flatten_args, path if dump_tensors else None)

    with open(os.path.join(output_dir, 'model.txt'), "w") as f:
        f.write(text_format.MessageToString(params))

    operations = [
        op.op.name if op.WhichOneof('op_type') == 'op' else op.fused_op.name
        for op in params.ops if op.op.op != 'nop'
    ]

    with open(os.path.join(output_dir, 'layers.txt'), 'w') as f:
        f.write('\n'.join(operations))

    if len(model.graph.nodes) < 10000:
        gen_compute_graph(model, os.path.join(output_dir, output_file))

    return params
